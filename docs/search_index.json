[["the-methodology.html", "Chapter 2 The Methodology 2.1 Setting up 2.2 Prior models and tuning 2.3 Backfitting with MCMC Algorithm 2.4 BART Probit for Classification 2.5 References", " Chapter 2 The Methodology 2.1 Setting up We consider the fundamental problem of making inference about an unknown function f that predicts an output Y using a \\(p\\)-dimensional vector of inputs \\(x = (x_1, ..., x_p)\\) when \\[Y = f(x) + \\epsilon, \\text{with } \\epsilon \\sim N(0, σ^2)\\] To do this, we consider modeling or at least approximating \\(f(x) = E(Y | x)\\), the mean of \\(Y\\) given \\(x\\), by a sum of \\(m\\) regression trees \\[f(x) \\approx h(x) ≡ \\sum_{j=1}^m g_j(x) \\text{ where each } g \\text{ denotes a regression tree and } g_j \\text{ denotes the } jth \\text{ tree}\\] Thus, we approximate \\(Y = f(x) + \\epsilon\\), with \\(\\epsilon \\sim N(0, \\sigma^2)\\) by a sum-of-trees model \\(Y = h(x) + \\epsilon, \\text{with }\\epsilon \\sim N(0, \\sigma^2)\\). Summarized more formally: \\[Y = f(x) +\\epsilon, \\text{with } \\epsilon \\sim N(0, \\epsilon^2)\\] \\[ f(x) = E(Y|x)\\] \\[ f(x) \\approx h(x) \\equiv \\sum^{m}_{j = 1} g_j(x)\\] where \\(g_j\\) denotes the \\(j_{th}\\) regression tree. So, we can equivalently write \\[ \\begin{align*} Y &amp;= h(x) +\\epsilon, \\text{with } \\epsilon \\sim N(0, \\sigma^2) \\\\ &amp; = \\sum^{m}_{j = 1} g_j(x) + \\epsilon \\end{align*} \\] Getting into more details, let \\(T\\) denote the set of all binary regression trees, and a full list of \\(m\\) trees can be denoted as the set \\(T = \\{T_1, T_2, T_3, ...\\}\\) For a specific binary regression tree \\(T_j\\) that has \\(b\\) terminal nodes, let \\(M_j = \\{M_1, M_2, M_3, ..., M_b\\}\\) denote the parameters for those \\(b\\) terminal nodes of this tree \\(T_j\\). Under this setting, \\[ \\begin{align*} Y &amp; = \\sum^{m}_{j = 1} g_j(x) + \\epsilon\\\\ &amp; = \\sum^{m}_{j = 1} g(x; T_j, M_j) + \\epsilon \\end{align*} \\] where \\(g(x; T_j, M_j)\\) is a function that assigns \\(\\mu_{ij}\\in M_j\\) to \\(x\\). The expected value of the output \\(Y\\) given \\(x\\), \\(E(Y | x)\\), is the sum of all the \\(\\mu_{ij}\\) values assigned to \\(x\\) by the functions \\(g(x; T_j, M_j)\\). When there are multiple trees (\\(m &gt; 1\\)), each \\(\\mu_{ij}\\) contributes only a part of \\(E(Y | x)\\), which is different from a single tree model. Each \\(\\mu_{ij}\\) represents a “main effect” if \\(g(x; T_j, M_j)\\) depends on only one component of \\(x\\) (a single variable), and an interaction effect if it depends on more than one component (more than one variable). Figure 2.1: A visual representation of \\(g(x;T_j, M_j)\\) . Square boxes represent the splitting rules, while circles denote the terminal nodes. Therefore, the sum-of-trees model can capture both main effects and interaction effects, with interaction effects of varying orders depending on the sizes of the trees. In the special case where every terminal node assignment depends on just a single component of \\(x\\), the sum-of-trees model simplifies to a simple additive model. Let \\(m\\) denote the number of trees we will build in a sum-of-trees model. For a fixed \\(m\\) , a sum of tree model is determined by \\(T = \\{T_1, T_2, T_3, ...\\}\\),\\(M_j = \\{M_1, M_2, M_3, ..., M_b\\}\\), and \\(\\sigma\\). Thus, all the priors that we need in this setting would be: Prior over \\((T_1, M_1), (T_2, M_2), (T_3, M_3), ... (T_m, M_m)\\) Prior over \\(\\sigma\\) For all those priors, we have their joint probability function as \\[ \\begin{align*} &amp; P((T_1, M_1), (T_2, M_2), (T_3, M_3), ... (T_m, M_m), \\sigma) \\\\ &amp; = \\prod^{m}_{j=1} (T_j, M_j)P(\\sigma) \\\\ &amp; = [\\prod^{m}_{j=1} P(M_j|T_j)P(T_j)]P(\\sigma), \\text{law of total probability} \\\\ &amp; = [\\prod^{m}_{j=1} P(\\mu_{1j}|T_{j})P(\\mu_{2j}|T_{j})...P(\\mu_{bj}|T_{j})]P(\\sigma), \\text{independence of parameters associated with nodes in a specific tree}\\\\ &amp; = [\\prod^{m}_{j=1} \\prod^{b_j}_{i=1} P(\\mu_{ij}|T_j)P(T_j)]P(\\sigma) \\end{align*} \\] Thus, after those calculation, we figured that there are only three priors that we need to think about. 2.2 Prior models and tuning 2.2.1 The \\(T_j\\) Prior The prior of \\(T_j\\) is specified by the three below aspects. The probability that a node at depth d (= 0, 1, 2, . . .) is nonterminal The distribution on the splitting variable assignments at each interior node The distribution on the splitting rule assignment in each interior node, conditional on the splitting variable \\[ \\begin{align*} (1) &amp; \\quad P(\\text{node of depth d is nonterminal}) = \\alpha(1 + d)^{-\\beta} \\text{ where } \\alpha \\in (0, 1), \\beta \\in [0, \\infty)\\\\ (2) &amp; \\quad \\text{splitting variable } p \\sim Unif()\\\\ (3) &amp; \\quad \\text{splitting rule | variable } p \\sim Unif() \\end{align*} \\] In order to keep the m trees small so they can be summed, we can choose to use values \\(\\alpha = 0.95\\) and \\(\\beta = 2\\) Figure 2.2: A single classification tree. Square boxes represent the splitting rules, while circles denote the terminal nodes. 2.2.2 The \\(\\mu_{ij} | T_j\\) prior The \\(\\mu_{ij} | T_j\\) prior uses the conjugate normal distribution: \\[ \\mu_{ij} | T_j \\sim N(\\mu_\\mu, \\sigma_\\mu^2) \\] The individual predictions \\(\\mu_{ij}\\) ’s are assumed to be independent and identically distributed (apriori i.i.d.) \\[ E(Y|x) \\sim N(m\\mu_\\mu, m\\sigma_\\mu^2) \\] Where it is highly probable for \\(E(Y|x)\\) to fall between \\(y_{min}\\) and \\(y_{max}\\) Because of this, we want to choose values of \\(\\mu_\\mu\\) and \\(\\sigma_\\mu\\) so that \\(E(Y|x) \\sim N(m\\mu_\\mu, m\\sigma_\\mu^2)\\) is most plausible in the interval of (\\(y_{min}\\), \\(y_{max}\\)). We can do this by choosing values of \\(\\mu_\\mu\\) and \\(\\sigma_\\mu\\) so that: \\(m\\mu_\\mu - k \\sqrt{m}\\sigma_\\mu = y_{min}\\) \\(m\\mu_\\mu + k \\sqrt{m}\\sigma_\\mu = y_{max}\\) From here, we can rescale \\(Y\\) so that our interval \\((y_{min}, y_{max})\\) is \\((-0.5, 0.5)\\) which gives \\[ \\mu_\\mu = 0 \\\\ \\sigma_\\mu = \\frac{0.5}{k\\sqrt{m}} \\] The value \\(k=2\\) is a good defualt choice as this would yield a 95% prior probability that \\(E(Y|x) \\in (y_{min}, y_{max})\\) 2.2.3 The \\(\\sigma\\) prior For \\(\\sigma^2\\) we use the inverse chi-square distribution: \\[ \\sigma^2 \\sim \\frac{\\nu\\lambda}{X^2\\nu} \\] The degrees of freedom (df) \\(\\nu \\in [3,10]\\) \\(\\lambda \\rightarrow\\) qth quantile of the prior on \\(\\sigma\\) is at \\(\\hat{\\sigma}\\) or: \\(P(\\sigma &lt; \\hat{\\sigma}) = q\\) The defualt can be \\((\\nu, q) = (3, 0.9)\\) which typically avoids extremes 2.2.4 Choosing m Remembering from before, \\(m\\) is the number of trees we are summing in our BART model A good default for this value is \\(m = 200\\) as the predictive performance of the algorithm improves starting at \\(m = 1\\) and levels at around 200 trees. 2.3 Backfitting with MCMC Algorithm Coming soon… need to go over and type up 2.4 BART Probit for Classification We have discussed BART model for continuous \\(Y\\), but BART can also be used for binary outcome \\(Y\\; (= 0\\text{ or }1)\\). 2.4.1 What is Probit/Probit Model? A probit model is a type of regression where the dependent variable \\(Y\\) can take only two values, for example married or not married, similar to logistic regression. However, probit model has different distribution than logistic regression. Specifically, we assume that the model takes the form \\[P(Y = 1 \\mid X) = \\Phi(X^\\text{T} \\beta)\\] where \\(P\\) is the probability and \\(\\Phi\\) is the cumulative distribution function (CDF) of the standard normal distribution \\(N(0,1)\\). 2.4.2 Setup BART for Classification Thus, BART model extends to be \\[p(x) \\equiv P [Y = 1 \\mid x] = \\Phi[G(x)]\\] where \\[G(x) \\equiv \\sum_{j=1}^{m} g(x; T_j, M_j)\\] , our sum of regression trees. 2.4.3 Modification of Regularization Priors BART extension to binary outcome requires imposing a regularization prior on \\(G(x)\\) and implementing a Bayesian backfitting algorithm for posterior computation. Fortunately, these can be obtained with only minor modifications of the methods for continuous \\(Y\\). Since we use probit model distribution, we implicitly assume \\(\\sigma = 1\\) and so only a prior on \\((T_1, M_1), \\ldots, (T_m, M_m)\\) is needed. Proceeding exactly as BART for continuous \\(Y\\), we consider a prior of the form: \\[ p((T_1,M_1),\\ldots,(T_m,M_m)) = \\prod_j \\left[ p(T_j) \\prod_i p(\\mu_{ij}|T_j) \\right] \\] where each tree prior \\(p(T_j)\\) is the same as tree prior for continuous \\(Y\\). So we need to figure out the updated prior on \\(p(\\mu_{ij}|T_j)\\). We choose \\(p(\\mu_{ij}|T_j)\\) to cover most of the relevant \\(p(x)\\) values by considering the interval \\((\\Phi[-3.0], \\Phi[3.0])\\), which is often practical. Following the motivation of \\(\\mu_{ij}\\) when having continuous \\(Y\\), we recommend the choice: \\[ \\mu_{ij} \\sim \\mathcal{N} \\left(0, \\sigma_{\\mu}^2 \\right) \\] where \\(\\sigma_{\\mu} = \\frac{3.0}{k\\sqrt{m}}\\), and \\(k\\) is chosen such that \\(G(x)\\) will likely fall within the interval \\((-3.0, 3.0)\\). Therefore, this prior can also limit the effect of the individual tree components of \\(G(x)\\) as \\(\\mu_{ij}\\) is shrinking towards zero. Thus, every small tree is a weak learner. As \\(k\\) and/or the number of trees \\(m\\) is increased, this prior will become tighter, leading to greater shrinkage applied to the \\(\\mu_{ij}\\). When choosing \\(k\\), statisticians found that values between 1 and 3 work well and they suggest using \\(k = 2\\) as the default choice, or determine it through cross-validation. 2.5 References Chipman, H. A., George, E. I., &amp; McCulloch, R. E. (2010). BART: Bayesian additive regression trees. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
