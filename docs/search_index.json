[["examples.html", "Chapter 3 Examples 3.1 Example 1: Regression 3.2 Example 2: Classification 3.3 References", " Chapter 3 Examples 3.1 Example 1: Regression # install.packages(&quot;countrycode&quot;) # install.packages(&quot;DALEXtra&quot;) # install.packages(&quot;devtools&quot;) # devtools::install_github(&#39;bbc/bbplot&#39;) library(tidyverse) library(tidymodels) library(countrycode) library(plotly) library(sysfonts) library(showtext) library(glue) library(scales) library(janitor) library(DALEXtra) library(bbplot) #Loading the datasets df_childhood &lt;- read_csv(&quot;https://raw.githubusercontent.com/mesdi/blog/main/childhood.csv&quot;) df_household &lt;- read_csv(&quot;https://raw.githubusercontent.com/mesdi/blog/main/household.csv&quot;) #Joining them by country and time df &lt;- df_childhood %&gt;% left_join(df_household, by = c(&quot;country&quot;, &quot;time&quot;)) %&gt;% na.omit() #Wrangling the dataset df_tidy &lt;- df %&gt;% mutate(household = round(household, 2), childhood = round(childhood, 2), age = str_replace(age, &quot;_&quot;, &quot;-&quot;), country_name = countrycode(country, &quot;genc3c&quot;, &quot;country.name&quot;) ) #Best 20 countries based on the household net worth in their last year df_tidy %&gt;% group_by(country) %&gt;% slice_max(time) %&gt;% slice_max(household, n=20) %&gt;% mutate(age = fct_reorder(age, childhood, .desc = TRUE), country_name = fct_reorder(country_name, household, .desc = TRUE)) %&gt;% ggplot(aes(x=country_name, y=childhood, fill = age, #Hover text of the barplot text = glue(&quot;{country}\\n%{childhood}\\n{age}\\nChildhood education&quot;))) + geom_col() + geom_line(aes(y=household/2, group = 1), color= &quot;skyblue&quot;, size=1) + #Adding the household net worth as a second(dual) y-axis scale_y_continuous(sec.axis = sec_axis(~.*2)) + scale_fill_viridis_d(name = &quot;&quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme_minimal() + theme( axis.text.x = element_text(angle = 60), axis.text.y = element_blank(), axis.text.y.right = element_blank(), panel.grid = element_blank(), legend.position = &quot;none&quot; ) -&gt; p #adding google font font_add_google(name = &quot;Henny Penny&quot;, family = &quot;henny&quot;) showtext_auto() #setting font family for ggplotly font &lt;- list( family= &quot;Henny Penny&quot;, size =5 ) #Plotly chart ggplotly(p, tooltip = c(&quot;text&quot;)) %&gt;% #Hover text of the line style(text = glue(&quot;{unique(p$data$country)}\\n%{unique(p$data$household)}\\nHousehold net worth&quot;),traces = 6) %&gt;% layout(font=font) head(df_tidy) ## # A tibble: 6 × 6 ## country age time childhood household country_name ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AUS AGE-3 2010 71.8 347. Australia ## 2 AUS AGE-3 2013 62.3 366. Australia ## 3 AUS AGE-3 2014 69.4 383. Australia ## 4 AUS AGE-3 2015 68.4 402. Australia ## 5 AUS AGE-3 2016 63.3 428. Australia ## 6 AUS AGE-3 2017 66.0 439. Australia #Splitting the data into train and test sets set.seed(1234) df_split &lt;- df_tidy %&gt;% #Converting the levels to variables for modeling pivot_wider(names_from = age, values_from = childhood) %&gt;% clean_names() %&gt;% na.omit() %&gt;% initial_split() df_train &lt;- training(df_split) df_test &lt;- testing(df_split) df_train ## # A tibble: 216 × 7 ## country time household country_name age_3 age_4 age_5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SVN 2015 324. Slovenia 82.8 89.3 91.8 ## 2 ITA 2014 575. Italy 92.0 96.0 97.0 ## 3 JPN 2012 571. Japan 78 93.8 94.6 ## 4 LUX 2011 391. Luxembourg 72.0 94.6 96.7 ## 5 ITA 2011 500. Italy 94.4 98.7 99.6 ## 6 ITA 2016 578. Italy 92.4 95.9 96.4 ## 7 ESP 2017 392. Spain 96.4 97.7 97.1 ## 8 GRC 2019 362. Greece 34.9 76.0 95.2 ## 9 DEU 2015 469. Germany 93.3 96.7 98.1 ## 10 LTU 2012 203. Lithuania 71.4 75 77.0 ## # ℹ 206 more rows #install.packages(&quot;dbarts&quot;) library(dbarts) #Preprocessing df_rec &lt;- recipe(household ~ age_3 + age_4 + age_5, data = df_train) #Modeling with BART df_spec &lt;- parsnip::bart() %&gt;% set_engine(&quot;dbarts&quot;) %&gt;% set_mode(&quot;regression&quot;) #Workflow df_wf &lt;- workflow() %&gt;% add_recipe(df_rec) %&gt;% add_model(df_spec) #cross-validation for resamples set.seed(12345) df_folds &lt;- vfold_cv(df_train) #Resampling for the accuracy metrics set.seed(98765) df_rs &lt;- df_wf %&gt;% fit_resamples(resamples = df_folds) #Computes the accuracy metrics collect_metrics(df_rs) ## # A tibble: 2 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 85.2 10 4.79 Preprocessor1_Model1 ## 2 rsq standard 0.528 10 0.0458 Preprocessor1_Model1 # A tibble: 2 x 6 # .metric .estimator mean n std_err .config # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #1 rmse standard 85.2 10 4.79 Preprocessor1_Model1 #2 rsq standard 0.528 10 0.0458 Preprocessor1_Model1 #Model tuning with grid search df_spec &lt;- parsnip::bart( trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune() ) %&gt;% set_engine(&quot;dbarts&quot;) %&gt;% set_mode(&quot;regression&quot;) #parameter object rf_param &lt;- workflow() %&gt;% add_model(df_spec) %&gt;% add_recipe(df_rec) %&gt;% extract_parameter_set_dials() %&gt;% finalize(df_train) #space-filling design with integer grid argument df_reg_tune &lt;- workflow() %&gt;% add_recipe(df_rec) %&gt;% add_model(df_spec) %&gt;% tune_grid( df_folds, grid = 20, param_info = rf_param, metrics = metric_set(rsq) ) #Selecting the best parameters according to the r-square rf_param_best &lt;- select_best(df_reg_tune, metric = &quot;rsq&quot;) %&gt;% select(-.config) #Final estimation with the object of best parameters final_df_wflow &lt;- workflow() %&gt;% add_model(df_spec) %&gt;% add_recipe(df_rec) %&gt;% finalize_workflow(rf_param_best) set.seed(12345) final_df_fit &lt;- final_df_wflow %&gt;% last_fit(df_split) #Computes final the accuracy metrics collect_metrics(final_df_fit) ## # A tibble: 2 × 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 88.6 Preprocessor1_Model1 ## 2 rsq standard 0.616 Preprocessor1_Model1 # A tibble: 2 x 4 # .metric .estimator .estimate .config # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; #1 rmse standard 84.5 Preprocessor1_Model1 #2 rsq standard 0.645 Preprocessor1_Model1 #Creating a preprocessed dataframe of the train dataset imp_data &lt;- df_rec %&gt;% prep() %&gt;% bake(new_data = NULL) #Final modeling with the best parameters df_spec_final &lt;- parsnip::bart( trees = 80, prior_terminal_node_coef = 0.884, prior_terminal_node_expo = 0.713 ) %&gt;% set_engine(&quot;dbarts&quot;) %&gt;% set_mode(&quot;regression&quot;) #building the explainer-object explainer_df &lt;- explain_tidymodels( df_spec_final %&gt;% fit(household ~ ., data = imp_data), data = imp_data %&gt;% select(-household), y = df_train$household, verbose = FALSE ) set.seed(1983) #calculates the variable-importance measure vip_df &lt;- model_parts( explainer = explainer_df, loss_function = loss_root_mean_square, B = 100, #the number of permutations type = &quot;difference&quot;, label =&quot;&quot; ) #Plotting ranking of the importance of explanatory variables plot(vip_df) + ggtitle(&quot;Mean variable-importance over 100 permutations&quot;, &quot;&quot;)+ theme(plot.title = element_text(hjust = 0.5, size = 20), axis.title.x = element_text(size=15), axis.text = element_text(size=15)) #Partial dependence profiles for 4-years old students set.seed(2403) pdp_age &lt;- model_profile(explainer_df, variables = &quot;age_4&quot;) as_tibble(pdp_age$agr_profiles) %&gt;% ggplot(aes(`_x_`, `_yhat_`)) + geom_line(data = as_tibble(pdp_age$cp_profiles), aes(x = age_4, group = `_ids_`), size = 0.5, alpha = 0.05, color = &quot;gray50&quot;)+ geom_line(color = &quot;midnightblue&quot;, size = 1.2, alpha = 0.8)+ bbc_style()+ labs(title= &quot;Household vs. AGE-4&quot;)+ theme(plot.title = element_text(hjust = 0.5), panel.grid.minor.x = element_line(color=&quot;grey&quot;)) 3.2 Example 2: Classification # Loading data library(palmerpenguins) data(penguins) head(penguins) ## # A tibble: 6 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; #Splitting the data into train and test sets set.seed(1234) penguin_split &lt;- penguins %&gt;% na.omit() %&gt;% initial_split() penguin_train &lt;- training(penguin_split) penguin_test &lt;- testing(penguin_split) #install.packages(&quot;dbarts&quot;) library(dbarts) #Preprocessing penguin_rec &lt;- recipe(species ~ bill_length_mm, bill_depth_mm, flipper_length_mm, data = penguin_train) #Modeling with BART penguin_spec &lt;- parsnip::bart() %&gt;% set_engine(&quot;dbarts&quot;) %&gt;% set_mode(&quot;classification&quot;) #Workflow penguin_wf &lt;- workflow() %&gt;% add_recipe(penguin_rec) %&gt;% add_model(penguin_spec) #cross-validation for resamples set.seed(12345) penguin_folds &lt;- vfold_cv(penguin_train) classification_metrics &lt;- metric_set(accuracy) # Resampling for the classification metrics penguin_rs &lt;- penguin_wf %&gt;% fit_resamples(resamples = penguin_folds, metrics = classification_metrics) # Compute the classification metrics collect_metrics(penguin_rs) ## # A tibble: 1 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy multiclass 0.618 10 0.0287 Preprocessor1_Model1 3.3 References Bayesian additive regression trees (BART) - bart. - bart • parsnip. (n.d.). https://parsnip.tidymodels.org/reference/bart.html Disci, S. (2022, December 8). The effect of childhood education on wealth: Modeling with bayesian additive regression trees (BART): R-bloggers. R. https://www.r-bloggers.com/2022/12/the-effect-of-childhood-education-on-wealth-modeling-with-bayesian-additive-regression-trees-bart/#google_vignette "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
