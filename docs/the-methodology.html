<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 The Methodology | Intro to Bayesian Addictive Regression Trees (BART)</title>
  <meta name="description" content="Chapter 2 The Methodology | Intro to Bayesian Addictive Regression Trees (BART)" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 The Methodology | Intro to Bayesian Addictive Regression Trees (BART)" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 The Methodology | Intro to Bayesian Addictive Regression Trees (BART)" />
  
  
  

<meta name="author" content="Jingyi Guan, Alayna Johnson, Wenxuan Zhu" />


<meta name="date" content="2024-04-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="examples.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">BART</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="the-methodology.html"><a href="the-methodology.html"><i class="fa fa-check"></i><b>2</b> The Methodology</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-methodology.html"><a href="the-methodology.html#setting-up"><i class="fa fa-check"></i><b>2.1</b> Setting up</a></li>
<li class="chapter" data-level="2.2" data-path="the-methodology.html"><a href="the-methodology.html#prior-models-and-tuning"><i class="fa fa-check"></i><b>2.2</b> Prior models and tuning</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-methodology.html"><a href="the-methodology.html#the-t_j-prior"><i class="fa fa-check"></i><b>2.2.1</b> The <span class="math inline">\(T_j\)</span> Prior</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-methodology.html"><a href="the-methodology.html#the-mu_ij-t_j-prior"><i class="fa fa-check"></i><b>2.2.2</b> The <span class="math inline">\(\mu_{ij} | T_j\)</span> prior</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-methodology.html"><a href="the-methodology.html#the-sigma-prior"><i class="fa fa-check"></i><b>2.2.3</b> The <span class="math inline">\(\sigma\)</span> prior</a></li>
<li class="chapter" data-level="2.2.4" data-path="the-methodology.html"><a href="the-methodology.html#choosing-m"><i class="fa fa-check"></i><b>2.2.4</b> Choosing m</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-methodology.html"><a href="the-methodology.html#backfitting-with-mcmc-algorithm"><i class="fa fa-check"></i><b>2.3</b> Backfitting with MCMC Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-methodology.html"><a href="the-methodology.html#the-algorithm-set-up"><i class="fa fa-check"></i><b>2.3.1</b> The algorithm set-up</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-methodology.html"><a href="the-methodology.html#posterior-inference-statistics"><i class="fa fa-check"></i><b>2.3.2</b> Posterior inference statistics</a></li>
<li class="chapter" data-level="2.3.3" data-path="the-methodology.html"><a href="the-methodology.html#conclusion"><i class="fa fa-check"></i><b>2.3.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-methodology.html"><a href="the-methodology.html#bart-probit-for-classification"><i class="fa fa-check"></i><b>2.4</b> BART Probit for Classification</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-methodology.html"><a href="the-methodology.html#what-is-probitprobit-model"><i class="fa fa-check"></i><b>2.4.1</b> What is Probit/Probit Model?</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-methodology.html"><a href="the-methodology.html#setup-bart-for-classification"><i class="fa fa-check"></i><b>2.4.2</b> Setup BART for Classification</a></li>
<li class="chapter" data-level="2.4.3" data-path="the-methodology.html"><a href="the-methodology.html#modification-of-regularization-priors"><i class="fa fa-check"></i><b>2.4.3</b> Modification of Regularization Priors</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-methodology.html"><a href="the-methodology.html#references"><i class="fa fa-check"></i><b>2.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="examples.html"><a href="examples.html"><i class="fa fa-check"></i><b>3</b> Examples</a>
<ul>
<li class="chapter" data-level="3.1" data-path="examples.html"><a href="examples.html#example-1-regression"><i class="fa fa-check"></i><b>3.1</b> Example 1: Regression</a></li>
<li class="chapter" data-level="3.2" data-path="examples.html"><a href="examples.html#example-2-classification"><i class="fa fa-check"></i><b>3.2</b> Example 2: Classification</a></li>
<li class="chapter" data-level="3.3" data-path="examples.html"><a href="examples.html#references-1"><i class="fa fa-check"></i><b>3.3</b> References</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intro to Bayesian Addictive Regression Trees (BART)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-methodology" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> The Methodology<a href="the-methodology.html#the-methodology" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="setting-up" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Setting up<a href="the-methodology.html#setting-up" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We consider the fundamental problem of making inference about an unknown
function f that predicts an output Y using a <span class="math inline">\(p\)</span>-dimensional vector of
inputs <span class="math inline">\(x = (x_1, ..., x_p)\)</span> when
<span class="math display">\[Y = f(x) + \epsilon, \text{with } \epsilon \sim N(0, σ^2)\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="image/simple-g.jpg" alt="A visual representation of $g(x;T_j, M_j)$ . Square boxes represent the splitting rules, while circles denote the terminal nodes. " width="80%" />
<p class="caption">
Figure 2.1: A visual representation of <span class="math inline">\(g(x;T_j, M_j)\)</span> . Square boxes represent the splitting rules, while circles denote the terminal nodes.
</p>
</div>
<p>To do this, we consider modeling or at least approximating <span class="math inline">\(f(x) = E(Y | x)\)</span>,
the mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span>, by a sum of <span class="math inline">\(m\)</span> regression trees
<span class="math display">\[f(x) \approx h(x) ≡ \sum_{j=1}^m g_j(x) \text{ where each } g \text{ denotes a regression tree and } g_j \text{ denotes the } jth \text{ tree}\]</span>
Thus, we approximate <span class="math inline">\(Y = f(x) + \epsilon\)</span>, with
<span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span> by a sum-of-trees model
<span class="math inline">\(Y = h(x) + \epsilon, \text{with }\epsilon \sim N(0, \sigma^2)\)</span>.
Summarized more formally:
<span class="math display">\[Y = f(x) +\epsilon, \text{with } \epsilon \sim N(0, \epsilon^2)\]</span>
<span class="math display">\[ f(x) = E(Y|x)\]</span> <span class="math display">\[ f(x) \approx h(x) \equiv \sum^{m}_{j = 1} g_j(x)\]</span>
where <span class="math inline">\(g_j\)</span> denotes the <span class="math inline">\(j_{th}\)</span> regression tree. So, we can
equivalently write
<span class="math display">\[ \begin{align*} Y &amp;= h(x) +\epsilon, \text{with } \epsilon \sim N(0, \sigma^2) \\ &amp; = \sum^{m}_{j = 1} g_j(x) + \epsilon \end{align*} \]</span>
Getting into more details, let <span class="math inline">\(T\)</span> denote the set of all binary
regression trees, and a full list of <span class="math inline">\(m\)</span> trees can be denoted as the set
<span class="math inline">\(T = \{T_1, T_2, T_3, ...\}\)</span> For a specific binary regression tree <span class="math inline">\(T_j\)</span>
that has <span class="math inline">\(b\)</span> terminal nodes, let <span class="math inline">\(M_j = \{M_1, M_2, M_3, ..., M_b\}\)</span>
denote the parameters for those <span class="math inline">\(b\)</span> terminal nodes of this tree <span class="math inline">\(T_j\)</span>.
Under this setting,
<span class="math display">\[ \begin{align*} Y &amp; = \sum^{m}_{j = 1} g_j(x) + \epsilon\\ &amp; = \sum^{m}_{j = 1} g(x; T_j, M_j) + \epsilon \end{align*} \]</span>
where <span class="math inline">\(g(x; T_j, M_j)\)</span> is a function that assigns <span class="math inline">\(\mu_{ij}\in M_j\)</span> to
<span class="math inline">\(x\)</span>. The expected value of the output <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span>, <span class="math inline">\(E(Y | x)\)</span>, is the
sum of all the <span class="math inline">\(\mu_{ij}\)</span> values assigned to <span class="math inline">\(x\)</span> by the functions
<span class="math inline">\(g(x; T_j, M_j)\)</span>. When there are multiple trees (<span class="math inline">\(m &gt; 1\)</span>), each
<span class="math inline">\(\mu_{ij}\)</span> contributes only a part of <span class="math inline">\(E(Y | x)\)</span>, which is different
from a single tree model. Each <span class="math inline">\(\mu_{ij}\)</span> represents a “main effect” if
<span class="math inline">\(g(x; T_j, M_j)\)</span> depends on only one component of <span class="math inline">\(x\)</span> (a single
variable), and an interaction effect if it depends on more than one
component (more than one variable).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="image/set-up.png" alt="A visual representation of the set-up of a sum-of-tree model." width="80%" />
<p class="caption">
Figure 2.2: A visual representation of the set-up of a sum-of-tree model.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="image/plug-x-in.png" alt="A visual representation of the prediction process of a sum-of-tree model." width="80%" />
<p class="caption">
Figure 2.3: A visual representation of the prediction process of a sum-of-tree model.
</p>
</div>
<p>Therefore, the sum-of-trees model can capture both main effects and
interaction effects, with interaction effects of varying orders
depending on the sizes of the trees. In the special case where every
terminal node assignment depends on just a single component of <span class="math inline">\(x\)</span>, the
sum-of-trees model simplifies to a simple additive model. Let <span class="math inline">\(m\)</span> denote
the number of trees we will build in a sum-of-trees model. For a fixed
<span class="math inline">\(m\)</span> , a sum of tree model is determined by
<span class="math inline">\(T = \{T_1, T_2, T_3, ...\}\)</span>,<span class="math inline">\(M_j = \{M_1, M_2, M_3, ..., M_b\}\)</span>, and
<span class="math inline">\(\sigma\)</span>. Thus, all the priors that we need in this setting would be:
Prior over <span class="math inline">\((T_1, M_1), (T_2, M_2), (T_3, M_3), ... (T_m, M_m)\)</span> Prior
over <span class="math inline">\(\sigma\)</span> For all those priors, we have their joint probability
function as
<span class="math display">\[ \begin{align*} &amp; P((T_1, M_1), (T_2, M_2), (T_3, M_3), ... (T_m, M_m), \sigma) \\ &amp; = \prod^{m}_{j=1} (T_j, M_j)P(\sigma) \\ &amp; = [\prod^{m}_{j=1} P(M_j|T_j)P(T_j)]P(\sigma), \text{law of total probability} \\ &amp; = [\prod^{m}_{j=1} P(\mu_{1j}|T_{j})P(\mu_{2j}|T_{j})...P(\mu_{bj}|T_{j})]P(\sigma), \text{independence of parameters associated with nodes in a specific tree}\\ &amp; = [\prod^{m}_{j=1} \prod^{b_j}_{i=1} P(\mu_{ij}|T_j)P(T_j)]P(\sigma) \end{align*} \]</span>
Thus, after those calculation, we figured that there are only three
priors that we need to think about.</p>
</div>
<div id="prior-models-and-tuning" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Prior models and tuning<a href="the-methodology.html#prior-models-and-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-t_j-prior" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> The <span class="math inline">\(T_j\)</span> Prior<a href="the-methodology.html#the-t_j-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The prior of <span class="math inline">\(T_j\)</span> is specified by the three below aspects.</p>
<ol style="list-style-type: decimal">
<li>The probability that a node at depth d (= 0, 1, 2, . . .) is
nonterminal</li>
<li>The distribution on the splitting variable assignments at each
interior node</li>
<li>The distribution on the splitting rule assignment in each interior
node, conditional on the splitting variable</li>
</ol>
<p><span class="math display">\[
\begin{align*}
(1) &amp; \quad P(\text{node of depth d is nonterminal}) = \alpha(1 + d)^{-\beta} \text{ where } \alpha \in (0, 1), \beta \in [0, \infty)\\
(2) &amp; \quad \text{splitting variable } p \sim Unif()\\
(3) &amp; \quad \text{splitting rule | variable } p \sim Unif()
\end{align*}
\]</span></p>
<p>In order to keep the m trees small so they can be summed, we can choose
to use values <span class="math inline">\(\alpha = 0.95\)</span> and <span class="math inline">\(\beta = 2\)</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="image/single_tree.jpeg" alt="A single classification tree. Square boxes represent the splitting rules, while circles denote the terminal nodes. " width="80%" />
<p class="caption">
Figure 2.4: A single classification tree. Square boxes represent the splitting rules, while circles denote the terminal nodes.
</p>
</div>
</div>
<div id="the-mu_ij-t_j-prior" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> The <span class="math inline">\(\mu_{ij} | T_j\)</span> prior<a href="the-methodology.html#the-mu_ij-t_j-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <span class="math inline">\(\mu_{ij} | T_j\)</span> prior uses the conjugate normal distribution:</p>
<p><span class="math display">\[
\mu_{ij} | T_j \sim N(\mu_\mu, \sigma_\mu^2)
\]</span></p>
<p>The individual predictions <span class="math inline">\(\mu_{ij}\)</span> ’s are assumed to be independent
and identically distributed (apriori i.i.d.)</p>
<p><span class="math display">\[
E(Y|x) \sim N(m\mu_\mu, m\sigma_\mu^2)
\]</span></p>
<p>Where it is highly probable for <span class="math inline">\(E(Y|x)\)</span> to fall between <span class="math inline">\(y_{min}\)</span> and
<span class="math inline">\(y_{max}\)</span></p>
<p>Because of this, we want to choose values of <span class="math inline">\(\mu_\mu\)</span> and <span class="math inline">\(\sigma_\mu\)</span>
so that <span class="math inline">\(E(Y|x) \sim N(m\mu_\mu, m\sigma_\mu^2)\)</span> is most plausible in
the interval of (<span class="math inline">\(y_{min}\)</span>, <span class="math inline">\(y_{max}\)</span>). We can do this by choosing
values of <span class="math inline">\(\mu_\mu\)</span> and <span class="math inline">\(\sigma_\mu\)</span> so that:</p>
<p><span class="math inline">\(m\mu_\mu - k \sqrt{m}\sigma_\mu = y_{min}\)</span></p>
<p><span class="math inline">\(m\mu_\mu + k \sqrt{m}\sigma_\mu = y_{max}\)</span></p>
<p>From here, we can rescale <span class="math inline">\(Y\)</span> so that our interval <span class="math inline">\((y_{min}, y_{max})\)</span>
is <span class="math inline">\((-0.5, 0.5)\)</span> which gives</p>
<p><span class="math display">\[
\mu_\mu = 0 \\
\sigma_\mu = \frac{0.5}{k\sqrt{m}}
\]</span></p>
<p>The value <span class="math inline">\(k=2\)</span> is a good defualt choice as this would yield a 95% prior
probability that <span class="math inline">\(E(Y|x) \in (y_{min}, y_{max})\)</span></p>
</div>
<div id="the-sigma-prior" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> The <span class="math inline">\(\sigma\)</span> prior<a href="the-methodology.html#the-sigma-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For <span class="math inline">\(\sigma^2\)</span> we use the inverse chi-square distribution:</p>
<p><span class="math display">\[
\sigma^2 \sim \frac{\nu\lambda}{X^2\nu}
\]</span></p>
<p>The degrees of freedom (df) <span class="math inline">\(\nu \in [3,10]\)</span></p>
<p><span class="math inline">\(\lambda \rightarrow\)</span> qth quantile of the prior on <span class="math inline">\(\sigma\)</span> is at
<span class="math inline">\(\hat{\sigma}\)</span> or: <span class="math inline">\(P(\sigma &lt; \hat{\sigma}) = q\)</span></p>
<p>The defualt can be <span class="math inline">\((\nu, q) = (3, 0.9)\)</span> which typically avoids extremes</p>
</div>
<div id="choosing-m" class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Choosing m<a href="the-methodology.html#choosing-m" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Remembering from before, <span class="math inline">\(m\)</span> is the number of trees we are summing in
our BART model</p>
<p>A good default for this value is <span class="math inline">\(m = 200\)</span> as the predictive performance
of the algorithm improves starting at <span class="math inline">\(m = 1\)</span> and levels at around 200
trees.</p>
<p><br />
<br />
</p>
</div>
</div>
<div id="backfitting-with-mcmc-algorithm" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Backfitting with MCMC Algorithm<a href="the-methodology.html#backfitting-with-mcmc-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-algorithm-set-up" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> The algorithm set-up<a href="the-methodology.html#the-algorithm-set-up" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Hastie and Tibshirani (2000) considered a similar application of the Gibbs sampler for posterior sampling for additive and generalized additive models with σ fixed, and showed how it was a stochastic generalization of the backfitting algorithm for such models. For this reason, Chipman et al. (2010) refer to their algorithm as backfitting MCMC.</p>
<p>Given the observed data <span class="math inline">\(y\)</span> and unknown parameters <span class="math inline">\((T_1, M_1), ..., (T_m, M_m), \sigma\)</span>, the posterior distribution of these parameters can be represented as <span class="math display">\[p((T_1, M_1), ..., (T_m, M_m), \sigma|y)\]</span>, which determine the sum-of-trees model.</p>
<p>Generally speaking, this backfitting algorithm iteratively updates each parameter in <span class="math inline">\((T_1, M_1), ...(T_m, M_m), \sigma\)</span> using the most current knowldge of the other parameters, which is just the idea of Gibbs sampler.</p>
<p>Specifically, <span class="math inline">\(T_{(j)}\)</span> represents all trees except <span class="math inline">\(T_j\)</span>, and <span class="math inline">\(M_{(j)}\)</span> represents all terminal nodes parameters except <span class="math inline">\(M_j\)</span>. In this Gibbs sampler, we loop from <span class="math inline">\(j = 1\)</span> to <span class="math inline">\(j = m\)</span> to draw <span class="math inline">\((T_j, M_j)\)</span> from <span class="math inline">\((T_{(j)}, M_{(j)}), \sigma, y\)</span>. That is, <span class="math display">\[(T_j, M_j) |T_{(j)}, M_{(j)}, \sigma, y\]</span>
We also need to update <span class="math inline">\(\sigma\)</span> based on the most recent knowledge of <span class="math inline">\((T_1, M_1), ...(T_m, M_m)\)</span>: <span class="math display">\[\sigma|T_1,...T_m, M_1, ...M_m, y\]</span>.</p>
<p>The posterior distribution <span class="math inline">\(p(T_j, M_j|T_{(j)}, M_{(j)}, \sigma, y)\)</span> depends on four things <span class="math inline">\(T_{(j)}, M_{(j)}, \sigma, y\)</span>. But it depends on three of them <span class="math inline">\(T_{(j)}, M_{(j)}, y\)</span> only through <span class="math display">\[R_j \equiv y-\sum_{k\neq j}g(x;T_k, M_k)\]</span>, which is an <span class="math inline">\(n\)</span>-vector where the <span class="math inline">\(j_{th}\)</span> entry records the discrepancy between the real value and the current estimate, where the current estimate is just the sum of the partial results gotten from all but the <span class="math inline">\(j_{th}\)</span> trees.</p>
<p>Consequently, drawing <span class="math inline">\((T_j, M_j)\)</span> given <span class="math inline">\(T_{(j)}, M_{(j)}, \sigma, y\)</span> can be regarded as equivalent to drawing from <span class="math inline">\(R_j, \sigma\)</span>, expressed as:</p>
<p><span class="math display">\[(T_j, M_j)|R_j, \sigma\]</span></p>
<p>Here, <span class="math inline">\(R_j\)</span> can be understood as the target value that the single tree model <span class="math inline">\(T_j\)</span> aims to get at, essentially acting as the “data” for this purpose. Formally, <span class="math display">\[R_j = g(x;T_j, M_j)+\epsilon\]</span>.</p>
<p>The calculation for <span class="math inline">\(p(T_j|R_j, \sigma)\)</span> is detailed as follows:
<span class="math display">\[
\begin{align*}
p(T_j|R_j, \sigma) &amp;= p(T_j)p(R_j|T_j,\sigma) \\
&amp;\propto p(T_j)\int p(R_j,M_j|T_j,\sigma)dM_j \\
&amp;= p(T_j)\int p(R_j|M_j, T_j,\sigma)p(M_j|T_j, \sigma)dM_j
\end{align*}
\]</span></p>
<p>Given the use of a conjugate prior for <span class="math inline">\(M_j\)</span>, we can derive <span class="math inline">\(p(T_j|R_j, \sigma)\)</span> in a closed form up to a normalization constant.</p>
<p>In summary, the whole backfitting algorithm utilizing the idea of Gibbs sampler is to successively draw <span class="math inline">\(T_1, M_1, ..., T_m, M_m, sigma\)</span> in each full iteration. Thus, each interation is consists of <span class="math inline">\(2m+1\)</span> steps:</p>
<p><span class="math display">\[
T_j|R_j, \sigma\\
M_j|T_j, R_j, \sigma \\
\sigma |T_1, M_1,...T_m, M_m,y, \sigma
\]</span>
Tree drawings <span class="math inline">\(T_j\)</span> are facilitated using the Metropolis-Hastings algorithm, which proposes new tree configurations based on the current tree through various modifications: growing a terminal node, pruning a pair of terminal nodes, changing a nonterminal rule, and swapping rules between parent and child nodes, with associated probabilities of 0.25, 0.25, 0.40, and 0.10 respectively.</p>
<p>Subsequently, <span class="math inline">\(M_j\)</span> is drawn independently for the terminal nodes <span class="math inline">\(\mu_{ij}\)</span> from a normal distribution, facilitating the calculation of the next residual <span class="math inline">\(R_{j+1}\)</span>, which is crucial for the upcoming draw of <span class="math inline">\(T_{j+1}\)</span>.</p>
</div>
<div id="posterior-inference-statistics" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Posterior inference statistics<a href="the-methodology.html#posterior-inference-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Every output from the algorithm can be used to build a tree-based model. Summing up all the tree outputs from each draw (indexed by j), we get a sequence of models that gradually approach the true underlying model, <span class="math inline">\(f(·)\)</span>. This is noted as:
<span class="math display">\[ f^*(·) = \sum_{j=1}^m g(·; T^*_j, M^*_j) \]</span></p>
<p>By running the algorithm for a sufficient number of iterations, especially after a preliminary ‘burn-in’ period, the sequence of <span class="math inline">\(f^{∗}\)</span> draws (like <span class="math inline">\(f^{∗}_1\)</span>, <span class="math inline">\(f^{∗}_2\)</span>, …, <span class="math inline">\(f^{∗}_K\)</span>) gives a good approximation of the posterior distribution of the true model <span class="math inline">\(f(·)\)</span>.</p>
<p>These tree-based model outputs can be used to estimate various statistical measures, like the mean or median of <span class="math inline">\(f(·)\)</span>, by averaging or finding the median of the post-burn-in samples.</p>
<p>To predict values or behaviors (<span class="math inline">\(Y\)</span>) at a new or existing data point (<span class="math inline">\(x\)</span>), you can use the average or median of these post-burn-in tree models.
To understand the reliability of these predictions at different values of <span class="math inline">\(x\)</span>, you look at how much the predictions vary among these models. This can be captured by calculating intervals (quantiles) around the predictions.
By observing which variables are most frequently used in the models (especially as the number of trees <span class="math inline">\(m\)</span> is reduced), BART helps identify the most important predictors for the response variable <span class="math inline">\(Y\)</span>. This process is effectively a competition among predictors to demonstrate their relevance to the model.</p>
</div>
<div id="conclusion" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Conclusion<a href="the-methodology.html#conclusion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In summary, the backfitting MCMC algorithm generates multiple potential models by simulating how trees would split based on data. These models converge over many iterations to a true model. By analyzing these models, one can make predictions, assess uncertainty, understand variable importance, and determine the impact of specific predictors on the outcome.<br />
<br />
</p>
</div>
</div>
<div id="bart-probit-for-classification" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> BART Probit for Classification<a href="the-methodology.html#bart-probit-for-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have discussed BART model for continuous <span class="math inline">\(Y\)</span>, but BART can also be
used for binary outcome <span class="math inline">\(Y\; (= 0\text{ or }1)\)</span>.</p>
<div id="what-is-probitprobit-model" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> What is Probit/Probit Model?<a href="the-methodology.html#what-is-probitprobit-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A probit model is a type of regression where the dependent variable <span class="math inline">\(Y\)</span>
can take only two values, for example married or not married, similar to
logistic regression. However, probit model has different distribution
than logistic regression. Specifically, we assume that the model takes
the form <span class="math display">\[P(Y = 1 \mid X) = \Phi(X^\text{T} \beta)\]</span> where <span class="math inline">\(P\)</span> is the
probability and <span class="math inline">\(\Phi\)</span> is the cumulative distribution function (CDF) of
the standard normal distribution <span class="math inline">\(N(0,1)\)</span>.</p>
</div>
<div id="setup-bart-for-classification" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Setup BART for Classification<a href="the-methodology.html#setup-bart-for-classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Thus, BART model extends to be
<span class="math display">\[p(x) \equiv P [Y = 1 \mid x] = \Phi[G(x)]\]</span> where
<span class="math display">\[G(x) \equiv \sum_{j=1}^{m} g(x; T_j, M_j)\]</span> , our sum of regression
trees.</p>
</div>
<div id="modification-of-regularization-priors" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Modification of Regularization Priors<a href="the-methodology.html#modification-of-regularization-priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>BART extension to binary outcome requires imposing a regularization
prior on <span class="math inline">\(G(x)\)</span> and implementing a Bayesian backfitting algorithm for
posterior computation. Fortunately, these can be obtained with only
minor modifications of the methods for continuous <span class="math inline">\(Y\)</span>. Since we use
probit model distribution, we implicitly assume <span class="math inline">\(\sigma = 1\)</span> and so only
a prior on <span class="math inline">\((T_1, M_1), \ldots, (T_m, M_m)\)</span> is needed. Proceeding
exactly as BART for continuous <span class="math inline">\(Y\)</span>, we consider a prior of the form: <span class="math display">\[
p((T_1,M_1),\ldots,(T_m,M_m)) = \prod_j \left[ p(T_j) \prod_i p(\mu_{ij}|T_j) \right]
\]</span> where each tree prior <span class="math inline">\(p(T_j)\)</span> is the same as tree prior for
continuous <span class="math inline">\(Y\)</span>. So we need to figure out the updated prior on
<span class="math inline">\(p(\mu_{ij}|T_j)\)</span>. We choose <span class="math inline">\(p(\mu_{ij}|T_j)\)</span> to cover most of the
relevant <span class="math inline">\(p(x)\)</span> values by considering the interval
<span class="math inline">\((\Phi[-3.0], \Phi[3.0])\)</span>, which is often practical. Following the
motivation of <span class="math inline">\(\mu_{ij}\)</span> when having continuous <span class="math inline">\(Y\)</span>, we recommend the
choice: <span class="math display">\[
\mu_{ij} \sim \mathcal{N} \left(0, \sigma_{\mu}^2 \right)
\]</span> where <span class="math inline">\(\sigma_{\mu} = \frac{3.0}{k\sqrt{m}}\)</span>, and <span class="math inline">\(k\)</span> is chosen such
that <span class="math inline">\(G(x)\)</span> will likely fall within the interval <span class="math inline">\((-3.0, 3.0)\)</span>.
Therefore, this prior can also limit the effect of the individual tree
components of <span class="math inline">\(G(x)\)</span> as <span class="math inline">\(\mu_{ij}\)</span> is shrinking towards zero. Thus,
every small tree is a weak learner. As <span class="math inline">\(k\)</span> and/or the number of trees
<span class="math inline">\(m\)</span> is increased, this prior will become tighter, leading to greater
shrinkage applied to the <span class="math inline">\(\mu_{ij}\)</span>. When choosing <span class="math inline">\(k\)</span>, statisticians
found that values between 1 and 3 work well and they suggest using
<span class="math inline">\(k = 2\)</span> as the default choice, or determine it through cross-validation.</p>
</div>
</div>
<div id="references" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> References<a href="the-methodology.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Chipman, H. A., George, E. I., &amp; McCulloch, R. E. (2010). BART:
Bayesian additive regression trees.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="examples.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/YOUR GITHUB USERNAME/YOUR REPO NAME/edit/main/02-method.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/YOUR GITHUB USERNAME/YOUR REPO NAME/blob/main/02-method.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
